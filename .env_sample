# Make sure you dont add single or double quotes in the variable values, example : 
# correct format is : OPENAI_BASE_URL=http://localhost:1234/v1
# but incorrect format is : OPENAI_BASE_URL="http://localhost:1234/v1"

OPENAI_BASE_URL=<your model provider base url ex. for openai it is https://api.openai.com/v1/ or for LM Studio it is http://localhost:1234/v1>
OPENAI_API_KEY=<your model provider api key, ex. for lm-studio lm-studio>
OPENAI_MODEL_NAME=<your model name ex. qwen3-14b>
LLM_MAX_PARALLEL_REQUESTS_BATCH_SIZE=<The max number of parallel requests to make while calling the LLM. This step is used in the emotion adding flow. Make sure your inference provider supports parallel inference for example. llama-cpp with --parallel flag. You can set its value to greater than or equal to 1>
TTS_BASE_URL=<Audio generator fastapi exposed openai compatible base url ex. http://localhost:8880/v1 for Kokoro or http://localhost:5005/v1 for Orpheus>
TTS_API_KEY=<specify if needed ex. not-needed>
TTS_MODEL=<TTS model to use. Choose either kokoro or orpheus>
NO_THINK_MODE=<whether to disable thinking mode in LLMs like Qwen3, R1 etc for faster inference. Takes in values of either true or false. Default is true>
TTS_MAX_PARALLEL_REQUESTS_BATCH_SIZE=<Choose the value based on this guide: https://github.com/prakharsr/audiobook-creator/?tab=readme-ov-file#parallel-batch-inferencing-of-audio-for-faster-audio-generation>